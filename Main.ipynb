{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #0000FF; color: white; padding: 10px; border-radius: 5px;\">\n",
    "  <h1>Team Info</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Name: Turbo Adventure\n",
    "### Team Members: Akshay, Louis, Reno, Gabrijel\n",
    "### Kaggle Usernames: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #0000FF; color: white; padding: 10px; border-radius: 5px;\">\n",
    "  <h1>1 Feature Engineering</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from librosa.feature import rhythm  \n",
    "from IPython.display import display\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load and Explore the Label Data\n",
    "\n",
    "In this step, we load the `labels_new.csv` file, which contains the known genres for the labeled audio files. \n",
    "Understanding the structure of this data will help us analyze the features of each genre later. \n",
    "\n",
    "We will check the number of entries, the column names, and some example rows to get an overview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genre Distribution Analysis\n",
    "\n",
    "To get an overview of the labeled dataset, we analyze the distribution of genres. Understanding how many samples belong to each genre helps us identify possible class imbalances, which can influence our machine learning models later.\n",
    "\n",
    "The following bar chart shows the distribution of the different music genres in the labeled dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each genre\n",
    "genre_counts = labels_df['genre'].value_counts()\n",
    "\n",
    "# Plot the genre distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=genre_counts.index, y=genre_counts.values, palette=\"viridis\")\n",
    "plt.title('Distribution of Music Genres in the Labeled Dataset')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Observation 1:** Some genres might have more samples than others, which can affect model training.\n",
    "- **Observation 2:** If the distribution is unbalanced, techniques like oversampling or undersampling might be needed.\n",
    "\n",
    "The visualization indicates how representative each genre is within the labeled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Audio File Analysis\n",
    "\n",
    "Before diving into feature extraction, we need to analyze some basic properties of the labeled audio files. \n",
    "This helps us understand the dataset and identify potential preprocessing needs. \n",
    "\n",
    "We will focus on the following key aspects:\n",
    "\n",
    "1. **Sampling Rate:** The number of samples per second, which affects the quality and size of the audio file.\n",
    "2. **Duration:** The total length of each audio file in seconds.\n",
    "3. **RMS Energy:** The average power of the signal, representing loudness.\n",
    "\n",
    "Analyzing these properties will give us insights into data consistency, possible outliers, and patterns that might affect further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the labeled audio files\n",
    "audio_folder = os.path.join(base_folder_path, 'labeled')\n",
    "\n",
    "# Function to extract basic audio properties\n",
    "def analyze_audio(file_path):\n",
    "    try:\n",
    "        # Load the audio file and get sampling rate\n",
    "        audio, sr = librosa.load(file_path, sr=None)\n",
    "        \n",
    "        # Extract duration and RMS energy\n",
    "        duration = librosa.get_duration(y=audio, sr=sr)\n",
    "        rms_energy = np.mean(librosa.feature.rms(y=audio))\n",
    "        \n",
    "        return sr, duration, rms_energy\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Analyze all audio files in the labeled folder\n",
    "audio_data = []\n",
    "\n",
    "for filename in labels_df['filename']:\n",
    "    file_path = os.path.join(audio_folder, filename)\n",
    "    sr, duration, rms_energy = analyze_audio(file_path)\n",
    "    \n",
    "    audio_data.append({\n",
    "        'filename': filename,\n",
    "        'sampling_rate': sr,\n",
    "        'duration': duration,\n",
    "        'rms_energy': rms_energy\n",
    "    })\n",
    "\n",
    "# Convert the extracted data into a DataFrame\n",
    "audio_features_df = pd.DataFrame(audio_data)\n",
    "\n",
    "# Display the first few rows to check the results\n",
    "audio_features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings from Audio Analysis\n",
    "\n",
    "Based on the extracted audio properties, we can observe the following:\n",
    "\n",
    "1. **Sampling Rate:** \n",
    "   - All audio files have the same sampling rate, ensuring consistency across the dataset.\n",
    "   - If different sampling rates were found, resampling might be required to maintain uniformity.\n",
    "\n",
    "2. **Duration:** \n",
    "   - The lengths of audio clips vary, which could affect feature extraction and model performance.\n",
    "   - Shorter clips might contain limited information, while longer clips could introduce noise.\n",
    "\n",
    "3. **RMS Energy:** \n",
    "   - There are noticeable differences in loudness across the clips.\n",
    "   - Genres like metal or hip-hop may have higher energy levels compared to classical or jazz.\n",
    "\n",
    "We will visualize these properties to better understand the distribution and check for potential anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Visualization of Audio Features\n",
    "\n",
    "To gain further insights into the labeled audio dataset, we visualize key properties extracted earlier:\n",
    "\n",
    "1. **Duration:** Understanding the length distribution of the audio clips helps us identify potential inconsistencies.\n",
    "2. **RMS Energy:** This feature gives an indication of the loudness of each audio file.\n",
    "3. **Sampling Rate:** Checking whether all files have the same sampling rate ensures consistency in the dataset.\n",
    "\n",
    "These visualizations will help us detect patterns, spot anomalies, and prepare for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "missing_values = audio_features_df.isnull().sum()\n",
    "\n",
    "# Summary statistics for numerical columns\n",
    "summary_stats = audio_features_df.describe()\n",
    "\n",
    "# Plot distributions of duration and RMS energy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Distribution of audio duration\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(audio_features_df['duration'], bins=20, kde=True, color='blue')\n",
    "plt.title('Distribution of Audio Duration')\n",
    "plt.xlabel('Duration (seconds)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Distribution of RMS energy\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(audio_features_df['rms_energy'], bins=20, kde=True, color='green')\n",
    "plt.title('Distribution of RMS Energy')\n",
    "plt.xlabel('RMS Energy')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Checking for unique values in sampling rate\n",
    "unique_sampling_rates = audio_features_df['sampling_rate'].unique()\n",
    "\n",
    "missing_values, summary_stats, unique_sampling_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings from the Visualization\n",
    "\n",
    "Based on the visualizations, we can draw the following conclusions:\n",
    "\n",
    "1. **Audio Duration:**\n",
    "   - The majority of audio clips fall within a specific duration range.\n",
    "   - A few outliers exist, which may require further inspection or trimming.\n",
    "\n",
    "2. **RMS Energy:**\n",
    "   - Some tracks have significantly higher or lower energy levels.\n",
    "   - This could indicate differences in genre characteristics or recording quality.\n",
    "\n",
    "3. **Sampling Rate:**\n",
    "   - All audio files share the same sampling rate, which is essential for maintaining uniformity in feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Feature Correlation Analysis\n",
    "\n",
    "To better understand relationships between the extracted features, we analyze the correlation between them.\n",
    "\n",
    "The correlation analysis helps in identifying redundant features and selecting the most relevant ones for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix for labeled data\n",
    "corr_matrix = audio_features_df.corr()\n",
    "\n",
    "# Plot the heatmap to visualize correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings from Feature Correlation Analysis\n",
    "\n",
    "- Features with high correlation values might contain redundant information and should be reviewed.\n",
    "- A strong correlation between different features can impact clustering performance.\n",
    "- Based on this analysis, we can decide whether to remove or combine features before clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Comparison Between Labeled and Unlabeled Data\n",
    "\n",
    "To ensure that the labeled and unlabeled datasets have similar characteristics, we compare their feature distributions.\n",
    "\n",
    "Identifying significant differences helps us decide if preprocessing adjustments are needed before applying clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to compare between labeled and unlabeled data\n",
    "features_to_compare = [\"Spectral Centroid\", \"Spectral Bandwidth\", \"RMS Amplitude\"]\n",
    "\n",
    "# Plot distribution comparisons\n",
    "plt.figure(figsize=(15, 6))\n",
    "for idx, feature in enumerate(features_to_compare, 1):\n",
    "    plt.subplot(1, 3, idx)\n",
    "    sns.kdeplot(labeled_audio_features_df[feature], label=\"Labeled\", fill=True, alpha=0.5)\n",
    "    sns.kdeplot(unlabeled_audio_features_df[feature], label=\"Unlabeled\", fill=True, alpha=0.5)\n",
    "    plt.title(feature)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings from Labeled vs. Unlabeled Data Comparison\n",
    "\n",
    "- **Spectral Features:** Differences between labeled and unlabeled data could indicate varying audio quality or recording environments.\n",
    "- **Energy Levels:** Large variations in RMS values may suggest differences in loudness, affecting clustering.\n",
    "- **Potential Adjustments:** If significant deviations exist, normalization or feature scaling might be necessary before clustering.\n",
    "\n",
    "Overall, this analysis ensures that both datasets share similar characteristics, improving the reliability of unsupervised learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Genre-based Feature Analysis\n",
    "\n",
    "Music genres can exhibit distinct characteristics in their audio features. \n",
    "\n",
    "By analyzing feature distributions across genres, we can identify patterns that may help improve clustering and genre classification.\n",
    "\n",
    "In this step, we visualize key features across different genres to explore these relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key features for genre comparison\n",
    "genre_features = [\"Spectral Centroid\", \"Spectral Bandwidth\", \"RMS Amplitude\"]\n",
    "\n",
    "# Create violin plots to compare feature distributions per genre\n",
    "plt.figure(figsize=(15, 6))\n",
    "for idx, feature in enumerate(genre_features, 1):\n",
    "    plt.subplot(1, 3, idx)\n",
    "    sns.violinplot(x=labeled_audio_features_df[\"genre\"], y=labeled_audio_features_df[feature])\n",
    "    plt.title(f'{feature} by Genre')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings from Genre-based Analysis\n",
    "\n",
    "- **Spectral Characteristics:**  \n",
    "  Some genres exhibit higher spectral centroid values, indicating brighter or more percussive content.\n",
    "\n",
    "- **Energy Levels:**  \n",
    "  Genres like rock and hip-hop might have higher RMS values compared to classical or jazz music.\n",
    "\n",
    "- **Implications for Clustering:**  \n",
    "  Feature overlap between genres may pose challenges in unsupervised learning, indicating the need for feature engineering or selection.\n",
    "\n",
    "Understanding these patterns will help improve clustering performance and feature selection strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT, 2024, Prompt 1: \"welke geluidsfeatures zijn er voor clustering van geluidsfragmenten\"\n",
    "# ChatGPT, 2024, Prompt 2: \"geef per feature aan welke library ik daarvoor kan gebruiken\"\n",
    "# ChatGPT, 2024, Prompt 3: \"laat voorbeeld code zien hoe deze features worden toegepast op een geluidsfragment, en vervolgens in een df wordt gezet\"\n",
    "# ChatGPT, 2024, Prompt 3: \"hoe pas ik oop toe op deze code\"\n",
    "# Link: https://chatgpt.com/share/677ee38d-fb54-8001-a50a-9856d52e22c9\n",
    "\n",
    "class AudioFeatureExtractor:\n",
    "    \"\"\"\n",
    "    A class to extract audio features from all .wav files in a given folder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, audio_folder):\n",
    "        \"\"\"\n",
    "        Initialize the AudioFeatureExtractor with the folder containing audio files.\n",
    "        \"\"\"\n",
    "        self.audio_folder = audio_folder\n",
    "        self.features_list = []\n",
    "\n",
    "    def process_audio_files(self):\n",
    "        \"\"\"\n",
    "        Iterate through all .wav files in the folder and extract features.\n",
    "        \"\"\"\n",
    "        for file_name in os.listdir(self.audio_folder):\n",
    "            if file_name.endswith(\".wav\"):  # Only process .wav files\n",
    "                file_path = os.path.join(self.audio_folder, file_name)\n",
    "                self._extract_features(file_name, file_path)\n",
    "\n",
    "    def _extract_features(self, file_name, file_path):\n",
    "        \"\"\"\n",
    "        Extract features from a single audio file and add them to the features list.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load the audio file\n",
    "            y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "            # Compute features\n",
    "            features = {\n",
    "                \"File Name\": file_name,\n",
    "                \"Zero Crossing Rate\": np.mean(librosa.feature.zero_crossing_rate(y)[0]),\n",
    "                \"RMS Amplitude\": np.mean(librosa.feature.rms(y=y)[0]),\n",
    "                \"Spectral Centroid\": np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)[0]),\n",
    "                \"Spectral Bandwidth\": np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]),\n",
    "                \"Spectral Roll-Off\": np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr)[0]),\n",
    "                \"Spectral Flatness\": np.mean(librosa.feature.spectral_flatness(y=y)[0]),\n",
    "                \"MFCC1\": np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)[0]),\n",
    "                \"MFCC2\": np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)[1]),\n",
    "                \"Chroma Features\": np.mean(librosa.feature.chroma_cqt(y=y, sr=sr)),\n",
    "                \"Tempo (BPM)\": rhythm.tempo(y=y, sr=sr)[0],\n",
    "            }\n",
    "\n",
    "            # Append features to the list\n",
    "            self.features_list.append(features)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    def get_features_dataframe(self):\n",
    "        \"\"\"\n",
    "        Convert the features list to a pandas DataFrame.\n",
    "        \"\"\"\n",
    "        return pd.DataFrame(self.features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Extract Features & Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Zero Crossing Rate</th>\n",
       "      <th>RMS Amplitude</th>\n",
       "      <th>Spectral Centroid</th>\n",
       "      <th>Spectral Bandwidth</th>\n",
       "      <th>Spectral Roll-Off</th>\n",
       "      <th>Spectral Flatness</th>\n",
       "      <th>MFCC1</th>\n",
       "      <th>MFCC2</th>\n",
       "      <th>Chroma Features</th>\n",
       "      <th>Tempo (BPM)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m00003.wav</td>\n",
       "      <td>0.120116</td>\n",
       "      <td>0.136221</td>\n",
       "      <td>2254.606986</td>\n",
       "      <td>2071.028440</td>\n",
       "      <td>4381.532206</td>\n",
       "      <td>0.009073</td>\n",
       "      <td>-82.511116</td>\n",
       "      <td>97.341721</td>\n",
       "      <td>0.571940</td>\n",
       "      <td>135.999178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m00012.wav</td>\n",
       "      <td>0.178108</td>\n",
       "      <td>0.217475</td>\n",
       "      <td>2908.300131</td>\n",
       "      <td>2286.252592</td>\n",
       "      <td>5405.816551</td>\n",
       "      <td>0.026373</td>\n",
       "      <td>-1.913298</td>\n",
       "      <td>72.686157</td>\n",
       "      <td>0.533881</td>\n",
       "      <td>103.359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m00013.wav</td>\n",
       "      <td>0.123423</td>\n",
       "      <td>0.029083</td>\n",
       "      <td>1952.607933</td>\n",
       "      <td>1865.996047</td>\n",
       "      <td>3779.909832</td>\n",
       "      <td>0.002211</td>\n",
       "      <td>-287.603699</td>\n",
       "      <td>101.758171</td>\n",
       "      <td>0.385155</td>\n",
       "      <td>95.703125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m00043.wav</td>\n",
       "      <td>0.126073</td>\n",
       "      <td>0.100411</td>\n",
       "      <td>2384.634958</td>\n",
       "      <td>2072.759900</td>\n",
       "      <td>4584.132502</td>\n",
       "      <td>0.008830</td>\n",
       "      <td>-120.147491</td>\n",
       "      <td>91.317215</td>\n",
       "      <td>0.557109</td>\n",
       "      <td>135.999178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m00044.wav</td>\n",
       "      <td>0.052088</td>\n",
       "      <td>0.018833</td>\n",
       "      <td>790.507005</td>\n",
       "      <td>900.409298</td>\n",
       "      <td>1159.412273</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>-437.604401</td>\n",
       "      <td>170.931534</td>\n",
       "      <td>0.332215</td>\n",
       "      <td>107.666016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>m00971.wav</td>\n",
       "      <td>0.091590</td>\n",
       "      <td>0.038885</td>\n",
       "      <td>1525.791709</td>\n",
       "      <td>1588.217850</td>\n",
       "      <td>2733.184659</td>\n",
       "      <td>0.006208</td>\n",
       "      <td>-246.158646</td>\n",
       "      <td>149.428665</td>\n",
       "      <td>0.414986</td>\n",
       "      <td>107.666016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>m00973.wav</td>\n",
       "      <td>0.161797</td>\n",
       "      <td>0.259782</td>\n",
       "      <td>3669.810467</td>\n",
       "      <td>3295.063699</td>\n",
       "      <td>7876.372739</td>\n",
       "      <td>0.088763</td>\n",
       "      <td>-8.573005</td>\n",
       "      <td>54.295788</td>\n",
       "      <td>0.475779</td>\n",
       "      <td>129.199219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>m00988.wav</td>\n",
       "      <td>0.167480</td>\n",
       "      <td>0.240724</td>\n",
       "      <td>3941.417697</td>\n",
       "      <td>3361.641125</td>\n",
       "      <td>8260.423162</td>\n",
       "      <td>0.089354</td>\n",
       "      <td>-56.153061</td>\n",
       "      <td>36.448502</td>\n",
       "      <td>0.480985</td>\n",
       "      <td>103.359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>m00991.wav</td>\n",
       "      <td>0.044217</td>\n",
       "      <td>0.017138</td>\n",
       "      <td>983.971719</td>\n",
       "      <td>1404.408040</td>\n",
       "      <td>1853.129475</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>-406.846680</td>\n",
       "      <td>163.081345</td>\n",
       "      <td>0.235405</td>\n",
       "      <td>117.453835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>m00995.wav</td>\n",
       "      <td>0.071629</td>\n",
       "      <td>0.066664</td>\n",
       "      <td>1253.032399</td>\n",
       "      <td>1371.258170</td>\n",
       "      <td>2292.769869</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>-223.187332</td>\n",
       "      <td>163.253967</td>\n",
       "      <td>0.344185</td>\n",
       "      <td>151.999081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      File Name  Zero Crossing Rate  RMS Amplitude  Spectral Centroid  \\\n",
       "0    m00003.wav            0.120116       0.136221        2254.606986   \n",
       "1    m00012.wav            0.178108       0.217475        2908.300131   \n",
       "2    m00013.wav            0.123423       0.029083        1952.607933   \n",
       "3    m00043.wav            0.126073       0.100411        2384.634958   \n",
       "4    m00044.wav            0.052088       0.018833         790.507005   \n",
       "..          ...                 ...            ...                ...   \n",
       "100  m00971.wav            0.091590       0.038885        1525.791709   \n",
       "101  m00973.wav            0.161797       0.259782        3669.810467   \n",
       "102  m00988.wav            0.167480       0.240724        3941.417697   \n",
       "103  m00991.wav            0.044217       0.017138         983.971719   \n",
       "104  m00995.wav            0.071629       0.066664        1253.032399   \n",
       "\n",
       "     Spectral Bandwidth  Spectral Roll-Off  Spectral Flatness       MFCC1  \\\n",
       "0           2071.028440        4381.532206           0.009073  -82.511116   \n",
       "1           2286.252592        5405.816551           0.026373   -1.913298   \n",
       "2           1865.996047        3779.909832           0.002211 -287.603699   \n",
       "3           2072.759900        4584.132502           0.008830 -120.147491   \n",
       "4            900.409298        1159.412273           0.000153 -437.604401   \n",
       "..                  ...                ...                ...         ...   \n",
       "100         1588.217850        2733.184659           0.006208 -246.158646   \n",
       "101         3295.063699        7876.372739           0.088763   -8.573005   \n",
       "102         3361.641125        8260.423162           0.089354  -56.153061   \n",
       "103         1404.408040        1853.129475           0.000302 -406.846680   \n",
       "104         1371.258170        2292.769869           0.000566 -223.187332   \n",
       "\n",
       "          MFCC2  Chroma Features  Tempo (BPM)  \n",
       "0     97.341721         0.571940   135.999178  \n",
       "1     72.686157         0.533881   103.359375  \n",
       "2    101.758171         0.385155    95.703125  \n",
       "3     91.317215         0.557109   135.999178  \n",
       "4    170.931534         0.332215   107.666016  \n",
       "..          ...              ...          ...  \n",
       "100  149.428665         0.414986   107.666016  \n",
       "101   54.295788         0.475779   129.199219  \n",
       "102   36.448502         0.480985   103.359375  \n",
       "103  163.081345         0.235405   117.453835  \n",
       "104  163.253967         0.344185   151.999081  \n",
       "\n",
       "[105 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ChatGPT, 2024, Prompt 1: \"welke geluidsfeatures zijn er voor clustering van geluidsfragmenten\"\n",
    "# ChatGPT, 2024, Prompt 2: \"geef per feature aan welke library ik daarvoor kan gebruiken\"\n",
    "# ChatGPT, 2024, Prompt 3: \"laat voorbeeld code zien hoe deze features worden toegepast op een geluidsfragment, en vervolgens in een df wordt gezet\"\n",
    "# ChatGPT, 2024, Prompt 3: \"hoe pas ik oop toe op deze code\"\n",
    "# Link: https://chatgpt.com/share/677ee38d-fb54-8001-a50a-9856d52e22c9\n",
    "\n",
    "# Define the folder containing audio files\n",
    "audio_folder_path = r\"muziek-genre-clustering-24-25\\unlabeled\"\n",
    "\n",
    "# Create an instance of the AudioFeatureExtractor class\n",
    "extractor = AudioFeatureExtractor(audio_folder=audio_folder_path)\n",
    "\n",
    "# Process the audio files\n",
    "extractor.process_audio_files()\n",
    "\n",
    "# Get the features as a DataFrame\n",
    "audio_features_df = extractor.get_features_dataframe()\n",
    "\n",
    "# Display the DataFrame\n",
    "display(audio_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Between Labeled and Unlabeled Data\n",
    "\n",
    "In this step, we compare the feature distributions of labeled and unlabeled datasets to identify potential differences. \n",
    "\n",
    "By visualizing the distributions, we can check if the data characteristics are similar or if adjustments are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature distributions\n",
    "features_to_compare = [\"Spectral Centroid\", \"Spectral Bandwidth\", \"RMS Amplitude\"]\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "for idx, feature in enumerate(features_to_compare, 1):\n",
    "    plt.subplot(1, 3, idx)\n",
    "    sns.kdeplot(labeled_audio_features_df[feature], label=\"Labeled\", fill=True, alpha=0.5)\n",
    "    sns.kdeplot(unlabeled_audio_features_df[feature], label=\"Unlabeled\", fill=True, alpha=0.5)\n",
    "    plt.title(feature)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings from Data Comparison\n",
    "\n",
    "- Some features show notable differences between labeled and unlabeled datasets, which could impact clustering performance.\n",
    "- If significant variations are detected, further data preprocessing such as scaling or feature selection may be required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correlation Analysis\n",
    "\n",
    "To better understand relationships between extracted features, we analyze their correlations. \n",
    "\n",
    "A correlation matrix helps identify redundant features and informs feature selection decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "corr_matrix = labeled_audio_features_df.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings from Correlation Analysis\n",
    "\n",
    "- Highly correlated features might introduce redundancy and should be considered for removal or dimensionality reduction.\n",
    "- Features with low correlation might provide unique information useful for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genre-based Feature Analysis\n",
    "\n",
    "Since audio features can vary significantly between genres, it's important to analyze the distribution of features across different genres.\n",
    "\n",
    "This analysis helps us identify trends and possible misclassifications by examining feature variations within each genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key features for genre comparison\n",
    "genre_features = [\"Spectral Centroid\", \"Spectral Bandwidth\", \"RMS Amplitude\"]\n",
    "\n",
    "# Create violin plots to compare feature distributions per genre\n",
    "plt.figure(figsize=(15, 6))\n",
    "for idx, feature in enumerate(genre_features, 1):\n",
    "    plt.subplot(1, 3, idx)\n",
    "    sns.violinplot(x=labeled_audio_features_df[\"genre\"], y=labeled_audio_features_df[feature])\n",
    "    plt.title(f'{feature} by Genre')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings from Genre-based Analysis\n",
    "\n",
    "- Some genres exhibit distinct trends in specific features (e.g., rock music might have higher RMS values than classical music).\n",
    "- Overlapping distributions could indicate that some features alone are not enough for genre differentiation.\n",
    "- These insights guide the selection of features for clustering and potential dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Features Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Crossing Rate\n",
    "The zero crossing rate is the rate at which a signal changes from positive to negative or vice versa and thus crossing the zero line in a given period. (11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMS Amplitude\n",
    "The RMS Amplitude is the Root Mean Square of all the measured amplitudes of a signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Centroid\n",
    "The Spectral Centroid indicates where the center of mass of the spectrum is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Bandwidth\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Spectral Bandwidth**\n",
    "#### **What it means:**\n",
    "Spectral bandwidth measures the spread or width of the frequency spectrum. It quantifies how \"wide\" or \"narrow\" the spectrum is and represents the range of frequencies that significantly contribute to the signal. A smaller bandwidth indicates that the signal energy is concentrated around a few frequencies, while a larger bandwidth means energy is spread across a broader frequency range.\n",
    "\n",
    "#### **How it's calculated:**\n",
    "- **Definition**: The spectral bandwidth is typically the standard deviation (or variance) of the power spectrum around its centroid (mean frequency).\n",
    "- **Formula**:\n",
    "\n",
    "$$\n",
    "\\text{Bandwidth} = \\sqrt{\\frac{\\sum (f - f_c)^2 \\cdot S(f)}{\\sum S(f)}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( f \\): frequency  \n",
    "- \\( f_c \\): spectral centroid  \n",
    "- \\( S(f) \\): spectral power (magnitude squared of the spectrum)  \n",
    "\n",
    "#### **What it signifies:**\n",
    "- A **low bandwidth** suggests the signal has tonal or harmonic content, like a pure tone or music.\n",
    "- A **high bandwidth** suggests noise-like signals or signals with many different frequency components.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Roll-Off\n",
    "### **2. Spectral Roll-Off**\n",
    "#### **What it means:**\n",
    "Spectral roll-off indicates the frequency below which a certain percentage (e.g., 85%) of the total spectral energy is concentrated. It essentially gives a threshold frequency that defines the \"bulk\" of the signal's energy, ignoring very high frequencies that may have minimal contributions.\n",
    "\n",
    "#### **How it's calculated:**\n",
    "- **Definition**: It is the frequency \\( f_r \\) where a certain proportion \\( p \\) (commonly 85%) of the total power in the spectrum lies below that frequency.\n",
    "- **Formula**:\n",
    "\n",
    "$$\n",
    "\\sum_{f=0}^{f_r} S(f) \\geq p \\cdot \\sum_{f=0}^{f_{\\text{max}}} S(f)\n",
    "$$\n",
    "\n",
    "#### **What it signifies:**\n",
    "- A **low roll-off frequency** suggests that most of the signal's energy is concentrated in the lower frequencies (e.g., a bass-heavy signal).\n",
    "- A **high roll-off frequency** suggests significant energy in higher frequencies (e.g., signals with sharp transients or high harmonics).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Flatness\n",
    "### **3. Spectral Flatness**\n",
    "#### **What it means:**\n",
    "Spectral flatness measures how \"flat\" or \"peaky\" a spectrum is. It compares the geometric mean to the arithmetic mean of the spectrum and indicates whether the spectrum resembles a noise-like signal (flat) or a tone-like signal (peaky).\n",
    "\n",
    "#### **How it's calculated:**\n",
    "- **Definition**: It is the ratio of the geometric mean to the arithmetic mean of the power spectrum.\n",
    "- **Formula**:\n",
    "\n",
    "$$\n",
    "\\text{Flatness} = \\frac{\\left( \\prod_{f=0}^{f_{\\text{max}}} S(f) \\right)^{1/N}}{\\frac{1}{N} \\sum_{f=0}^{f_{\\text{max}}} S(f)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( S(f) \\): spectral power at frequency \\( f \\)  \n",
    "- \\( N \\): total number of frequency bins in the spectrum  \n",
    "\n",
    "#### **What it signifies:**\n",
    "- A **flat spectrum** (value near 1) suggests white noise or a similar signal where energy is evenly distributed across frequencies.\n",
    "- A **low flatness** (value near 0) indicates the presence of dominant tonal components, such as those in harmonic sounds or pure tones.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Interpretations**:\n",
    "\n",
    "| **Feature**         | **High Value Indicates**                          | **Low Value Indicates**                       |\n",
    "|----------------------|--------------------------------------------------|-----------------------------------------------|\n",
    "| **Spectral Bandwidth** | Broad, noisy, or transient signals              | Tonal, harmonic, or narrow-band signals       |\n",
    "| **Spectral Roll-Off** | Energy in higher frequencies                     | Energy concentrated in lower frequencies      |\n",
    "| **Spectral Flatness** | Noise-like, evenly distributed spectrum          | Tonal, peaky spectrum                         |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFCCs\n",
    "### **X. MFCCs**\n",
    "#### **What it means:**\n",
    "\n",
    "MFCCs (Mel-frequency Cepstral Coefficients) describe the short-term power spectrums of audio, which explain the most noticeable attributes of verbal communication. Applied in the real world, MFCCs are commonly used to distinguish between persons by speech.\n",
    "\n",
    "#### **How it's calculated:**\n",
    "\n",
    "There is not only one single way to calculate MFCCs, but usually, these steps are followed (GeeksforGeeks, 2024):\n",
    "\n",
    "- **Pre-emphasize the signal:** Stabilizing the spectrum by raising the frequency.\n",
    "- **Framing:** Splitting the audio into overlapping chunks.\n",
    "- **Windowing:** A window function is used to mitigate the edge effect of framing.\n",
    "- **FFT (Fast Fourier Transform):** The Fourier Transform converts time domain frames to frequency domain frames to get spectral representations.\n",
    "- **Mel-filterbank:** Separates spectral representations into frequency bands based on the Mel scale, prioritizing crucial frequencies.\n",
    "- **Logarithm:** The logarithm is calculated to better represent loudness perception.\n",
    "- **DCT:** After minimizing duplicate coefficients, the MFCCs are obtained.\n",
    "\n",
    "The rows of the MFCC matrix capture these features:\n",
    "\n",
    "- MFCC1:\n",
    "- MFCC2:\n",
    "\n",
    "#### **What it signifies:**\n",
    "\n",
    "- A **low MFCC1**: Indicates...\n",
    "- A **high MFCC1**: Indicates...\n",
    "- A **low MFCC2**: Indicates...\n",
    "- A **high MFCC2**: Indicates..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma Features\n",
    "\n",
    "Chroma features are widely used in music analysis and processing. They capture the harmonic content of a piece by summarizing the energy distribution across the twelve pitch classes (C, C#, D, etc.) within an octave. This allows for tonal comparisons irrespective of the octave.\n",
    "\n",
    "#### How do chroma features work?\n",
    "The extraction involves:\n",
    "\n",
    "1. **Frame segmentation:** Divide the audio signal into overlapping frames.\n",
    "2. **Frequency transformation:** Use **Short-Time Fourier Transform (STFT)** to convert the audio from the time to the frequency domain.\n",
    "3. **Pitch class aggregation:** Sum energy levels across all octaves for each pitch class, creating a 12-dimensional chroma vector.\n",
    "\n",
    "This process creates a \"fingerprint\" of the music's harmonic structure, enabling robust tonal analysis.\n",
    "\n",
    "#### Example: Computing Chroma Features\n",
    "For a sine wave at 440 Hz (A4 note), sampled at 44.1 kHz:\n",
    "\n",
    "1. Perform an STFT to get the frequency spectrum.\n",
    "2. Identify that 440 Hz corresponds to pitch class \"A.\"\n",
    "3. Assign energy to \"A\" in the chroma vector.\n",
    "\n",
    "Example output:\n",
    "`[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]`\n",
    "\n",
    "#### Applications and importance\n",
    "- **Chord recognition:** Identifies harmonic progressions.\n",
    "- **Genre classification:** Useful for complex harmonic structures.\n",
    "- **Music retrieval:** Helps identify similar tracks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tempo (BPM)\n",
    "\n",
    "Tempo refers to the speed or pace of a piece of music, measured in beats per minute (BPM). It reflects the rhythmic structure and is critical for understanding a song’s energy, danceability, and emotional impact. As Müller (2015) notes, tempo is one of the most intuitive features for listeners and a key determinant in defining musical genres. Fast tempos are common in energetic genres like electronic dance music (EDM), while slower tempos dominate ballads and blues.\n",
    "\n",
    "#### How is tempo calculated?\n",
    "The process of tempo extraction typically involves:\n",
    "1. **Onset detection:** Analyze changes in the amplitude of the audio signal to identify beats or note beginnings.\n",
    "2. **Periodic analysis:** Use techniques like **autocorrelation** or **Fourier Transform** to detect repeating patterns in the beats.\n",
    "3. **Beat frequency estimation:** Measure the time interval between beats and convert it into BPM.\n",
    "\n",
    "#### Example: Estimating Tempo\n",
    "Consider an audio track where beats occur every 0.5 seconds:\n",
    "1. Measure the time interval between beats: \\( T = 0.5 \\, \\text{s} \\).\n",
    "2. Calculate the tempo using the formula:  \n",
    "   $$\\text{BPM} = \\frac{60}{T} = \\frac{60}{0.5} = 120 \\, \\text{BPM}.$$\n",
    "\n",
    "This indicates the track has a tempo of 120 BPM, which is typical for many pop and dance tracks (Tzanetakis & Cook, 2002).\n",
    "\n",
    "#### Applications and importance\n",
    "- **Genre differentiation:** Tempo is a distinguishing feature in many genres, such as EDM (typically 120–140 BPM) versus ballads (50–70 BPM) (Müller, 2015).\n",
    "- **Music recommendation:** Tracks with similar tempos are often grouped together in playlists for specific activities, like workouts or relaxation (Wikipedia contributors, n.d.-b).\n",
    "- **Rhythmic analysis:** Provides insights into the overall \"feel\" or \"groove\" of a piece, which is essential for understanding its style and energy (Tzanetakis & Cook, 2002)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #0000FF; color: white; padding: 10px; border-radius: 5px;\">\n",
    "  <h1>2 Unsupervised Learning</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 K-Means\n",
    "\n",
    "K-Means clustering is an unsupervised machine learning algorithm used for grouping data into clusters. The number of clusters \\\\( K \\\\) must be specified beforehand.\n",
    "\n",
    "### How it works\n",
    "\n",
    "1. **Initialization:**  \n",
    "   - Choose \\\\( K \\\\) clusters.\n",
    "   - Randomly initialize \\\\( K \\\\) centroids.\n",
    "\n",
    "2. **Assigning Step:**  \n",
    "   - Assign each point to the nearest centroid using Euclidean distance:\n",
    "\n",
    "   \\\\[\n",
    "   Distance(x_i, c_k) = \\\\sqrt{\\\\sum_{j=1}^n (x_{ij} - c_{kj})^2}\n",
    "   \\\\]\n",
    "\n",
    "3. **Recomputing Step:**  \n",
    "   - Recalculate centroids by averaging points in each cluster.\n",
    "\n",
    "4. **Repeat and minimize variation:**  \n",
    "   - Repeat steps until centroids stabilize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 DBSCAN\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm introduced by Ester et al. (1996). It identifies clusters based on the density of data points in a given region and treats sparsely populated regions as noise or outliers. Unlike centroid-based algorithms such as K-Means, DBSCAN does not require the number of clusters to be specified beforehand. Instead, it uses two parameters, $\\varepsilon$ (epsilon) and $\\text{minPts}$, to determine cluster structure.\n",
    "\n",
    "This algorithm is particularly suitable for datasets where clusters exhibit irregular shapes and varying densities. In the context of this project, DBSCAN can be advantageous because musical genres may have complex, overlapping distributions in the feature space. Furthermore, DBSCAN’s ability to detect noise could help exclude outliers or unusual audio fragments that do not belong to any genre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Algorithmic Principles**\n",
    "\n",
    "DBSCAN uses the following key concepts to identify clusters:\n",
    "\n",
    "1. **Neighborhood definition:**\n",
    "   For a point \\(p\\), its \\(\\varepsilon\\)-neighborhood is defined as:\n",
    "   \\[\n",
    "   N_{\\varepsilon}(p) = \\{q \\in D \\,|\\, \\|p - q\\| \\leq \\varepsilon\\}\n",
    "   \\]\n",
    "   Here, \\(D\\) represents the dataset, and \\(\\|p - q\\|\\) is the distance between points \\(p\\) and \\(q\\) (commonly Euclidean distance).\n",
    "\n",
    "2. **Core points:**\n",
    "   A point \\(p\\) is classified as a core point if it satisfies:\n",
    "   \\[\n",
    "   |N_{\\varepsilon}(p)| \\geq \\text{minPts}\n",
    "   \\]\n",
    "\n",
    "3. **Border points:**\n",
    "   A point \\(p\\) is classified as a border point if it lies within the \\(\\varepsilon\\)-neighborhood of a core point but does not itself satisfy the core point condition.\n",
    "\n",
    "4. **Noise points:**\n",
    "   Points that are neither core points nor reachable from any core points are classified as noise.\n",
    "\n",
    "5. **Cluster formation:**\n",
    "   Clusters are formed by expanding the neighborhoods of core points. A core point can connect to other core points and border points, creating dense regions that define a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Algorithm Steps**\n",
    "\n",
    "1. Begin with an unvisited point \\(p\\).\n",
    "2. Determine if \\(p\\) is a core point:\n",
    "   - If \\(p\\) is a core point, start a new cluster.\n",
    "   - Include all points within the \\(\\varepsilon\\)-neighborhood of \\(p\\) in the cluster.\n",
    "   - Recursively check all core points in the neighborhood and add their connected points.\n",
    "3. If \\(p\\) is not a core point but lies in the neighborhood of another core point, classify it as a border point.\n",
    "4. If \\(p\\) is neither a core nor a border point, classify it as noise.\n",
    "5. Repeat until all points in the dataset have been visited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Advantages and Limitations**\n",
    "\n",
    "**Advantages:**\n",
    "- DBSCAN can identify clusters of arbitrary shapes, which is beneficial when analyzing datasets with irregular patterns, such as musical genres.\n",
    "- The algorithm automatically detects noise and excludes it from the clustering process.\n",
    "- There is no need to predefine the number of clusters, making it suitable for exploratory tasks.\n",
    "\n",
    "**Limitations:**\n",
    "- The algorithm is sensitive to the choice of \\(\\varepsilon\\) and \\(\\text{minPts}\\), and inappropriate parameter values can lead to over-clustering or under-clustering.\n",
    "- DBSCAN struggles with datasets that contain clusters of varying densities, as this can affect neighborhood detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Application Example**\n",
    "\n",
    "This example demonstrates the application of DBSCAN on a two-dimensional synthetic dataset that mimics the feature space of audio fragments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "dense_cluster = np.random.randn(100, 2) * 0.5 + [2, 2]\n",
    "sparse_cluster = np.random.randn(50, 2) * 0.8 + [-2, -2]\n",
    "noise = np.random.uniform(-6, 6, (20, 2))\n",
    "data = np.vstack([dense_cluster, sparse_cluster, noise])\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.8, min_samples=5)\n",
    "labels = dbscan.fit_predict(data)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "unique_labels = set(labels)\n",
    "for label in unique_labels:\n",
    "    cluster_points = data[labels == label]\n",
    "    if label == -1:\n",
    "        plt.scatter(cluster_points[:, 0], cluster_points[:, 1], color='red', label='Noise')\n",
    "    else:\n",
    "        plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {label}')\n",
    "plt.title('DBSCAN Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "DBSCAN is a suitable algorithm for this project because of its ability to identify clusters of arbitrary shapes and handle noise effectively. Unlike centroid-based methods such as K-Means, DBSCAN does not assume spherical clusters and is well-suited for datasets with irregular patterns (Ester et al., 1996). This flexibility is particularly advantageous when clustering musical genres, as the feature space may exhibit overlapping or complex structures.\n",
    "\n",
    "Moreover, DBSCAN's robustness against noise allows it to exclude outliers from the clustering process, making it effective for detecting unusual or out-of-distribution audio fragments (Pedregosa et al., 2011). However, the algorithm is sensitive to the choice of its parameters ($\\varepsilon$ and $\\text{minPts}$), which require careful tuning based on the dataset's density distribution.\n",
    "\n",
    "Despite these challenges, DBSCAN remains a strong candidate for exploratory clustering in this project due to its adaptability and lack of reliance on predefined cluster counts. Future steps should include parameter optimization and comparison with other clustering methods to assess its performance comprehensively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Gaussian Mixture Models (GMM)\n",
    "\n",
    "GMM is a soft clustering model that assigns each observation a probability of belonging to a specific cluster. In this algorithm, each cluster is represented by a normal (Gaussian) distribution with the mean and covariance matrix as metrics.\n",
    "\n",
    "This model utilizes the Expectation-Maximization (EM) method, which is split into the following phases:\n",
    "\n",
    "- **Initialize phase:**  \n",
    "  The mean, the covariance matrix, and the mixing coefficients of every Gaussian distribution (cluster) are assumed.\n",
    "\n",
    "- **Expectation phase:**  \n",
    "  The observations are assigned to Gaussian distributions (clusters) based on the calculated probability that they belong to them.\n",
    "\n",
    "- **Maximization phase:**  \n",
    "  The mean, the covariance matrix, and the mixing coefficients of every Gaussian distribution (cluster) are calculated with the assigned observations. \n",
    "\n",
    "The Expectation and Maximization phases are reiterated until the model can no longer be notably improved.\n",
    "\n",
    "To calculate the probability that an observation belongs to a cluster, the following probability density function (PDF) formula is used (What Is Gaussian Mixture Model | Deepchecks, 2023):\n",
    "\n",
    "$$\n",
    "\\text{pdf}(x) = \\sum_{k=1}^{K} \\pi_k \\cdot N(x|\\mu_k, \\Sigma_k)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( \\pi_k \\) = Mixing coefficient  \n",
    "- \\( \\mu_k \\) = Mean vector  \n",
    "- \\( \\Sigma_k \\) = Covariance matrix  \n",
    "- \\( N(x|\\mu_k, \\Sigma_k) \\) = Probability density function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 GMM CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #0000FF; color: white; padding: 10px; border-radius: 5px;\">\n",
    "  <h1>3 Findings and Conclusion</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #0000FF; color: white; padding: 10px; border-radius: 5px;\">\n",
    "  <h1>4 Reference List</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Sources\n",
    "\n",
    "* ChatGPT, 2024, Prompt 1: \"welke geluidsfeatures zijn er voor clustering van geluidsfragmenten\", (https://chatgpt.com/share/677ee38d-fb54-8001-a50a-9856d52e22c9)\n",
    "\n",
    "* ChatGPT, 2024, Prompt 2: \"geef per feature aan welke library ik daarvoor kan gebruiken\", (https://chatgpt.com/share/677ee38d-fb54-8001-a50a-9856d52e22c9)\n",
    "\n",
    "* ChatGPT, 2024, Prompt 3: \"laat voorbeeld code zien hoe deze features worden toegepast op een geluidsfragment, en vervolgens in een df wordt gezet\", (https://chatgpt.com/share/677ee38d-fb54-8001-a50a-9856d52e22c9)\n",
    "* ChatGPT, 2024, Prompt 4: \"hoe pas ik oop toe op deze code\", (https://chatgpt.com/share/677ee38d-fb54-8001-a50a-9856d52e22c9)\n",
    "\n",
    "- GeeksforGeeks. (2024, June 26). *Mel-frequency Cepstral Coefficients (MFCC) for Speech Recognition.*  \n",
    "  [GeeksforGeeks](https://www.geeksforgeeks.org/mel-frequency-cepstral-coefficients-mfcc-for-speech-recognition/)\n",
    "\n",
    "- Deepchecks. (2023, January 23). *What is Gaussian Mixture Model.*  \n",
    "  [Deepchecks](https://www.deepchecks.com/glossary/gaussian-mixture-model/)\n",
    "\n",
    "\n",
    "- Wikipedia contributors. (n.d.-a). Chroma feature. In *Wikipedia*. Retrieved January 12, 2025, from  \n",
    "   [https://en.wikipedia.org/wiki/Chroma_feature](https://en.wikipedia.org/wiki/Chroma_feature)\n",
    "\n",
    "-  Wikipedia contributors. (n.d.-b). Tempo. In *Wikipedia*. Retrieved January 12, 2025, from  \n",
    "   [https://en.wikipedia.org/wiki/Tempo](https://en.wikipedia.org/wiki/Tempo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Literature\n",
    "\n",
    "- Ester, M., Kriegel, H.-P., Sander, J., & Xu, X. (1996). A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. *Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining (KDD)*, 226–231.  \n",
    "  Retrieved from [https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf](https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf)\n",
    "\n",
    "- Müller, M. (2015). *Fundamentals of Music Processing: Audio, Analysis, Algorithms, Applications*. Springer Verlag.  \n",
    "   This comprehensive textbook covers key topics in music information retrieval, including chroma features and tempo analysis.\n",
    "\n",
    "- Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... & Duchesnay, É. (2011). Scikit-learn: Machine Learning in Python. *Journal of Machine Learning Research*, 12, 2825–2830.  \n",
    "  Retrieved from [https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html](https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html)\n",
    "\n",
    "-  Tzanetakis, G., & Cook, P. (2002). Musical genre classification of audio signals. *IEEE Transactions on Speech and Audio Processing*, 10(5), 293–302. DOI: [10.1109/TSA.2002.800560](https://doi.org/10.1109/TSA.2002.800560)\n",
    "\n",
    "### Yet to be documented sources\n",
    "\n",
    "- [K-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
